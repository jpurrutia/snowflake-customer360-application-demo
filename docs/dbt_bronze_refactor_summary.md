# dbt Bronze Layer Refactoring - Summary

## What Changed

We refactored the data ingestion pipeline to use **dbt for Bronze layer management** instead of manual SQL scripts.

---

## Before (Old Architecture)

```
CUSTOMERS:
  Snowpark Procedure â†’ Direct Write â†’ BRONZE.RAW_CUSTOMERS table

TRANSACTIONS:
  S3 CSV â†’ Manual SQL Script (COPY INTO) â†’ BRONZE.BRONZE_TRANSACTIONS table

dbt Pipeline: Starts at SILVER (staging models)
```

**Problems:**
- âŒ Bronze tables managed outside dbt (no lineage)
- âŒ Manual SQL scripts required
- âŒ Inconsistent patterns (direct write vs stage)
- âŒ No dbt tests on Bronze layer
- âŒ No orchestration for Bronze ingestion

---

## After (New Architecture)

```
CUSTOMERS:
  Snowpark Procedure â†’ @customer_data_stage (Parquet) â†’ dbt Bronze Model â†’ BRONZE.RAW_CUSTOMERS

TRANSACTIONS:
  S3 CSV â†’ @transaction_stage_historical (GZIP) â†’ dbt Bronze Model â†’ BRONZE.RAW_TRANSACTIONS

dbt Pipeline: BRONZE â†’ SILVER â†’ GOLD (full lineage)
```

**Benefits:**
- âœ… dbt manages entire pipeline (Bronze â†’ Gold)
- âœ… Single command: `dbt run` (no manual scripts)
- âœ… Consistent stage â†’ COPY INTO pattern
- âœ… Full data lineage in dbt DAG
- âœ… dbt tests on Bronze layer
- âœ… Incremental loads built-in

---

## Files Created

### 1. Snowflake Setup Scripts
- **`/snowflake/setup/09_create_internal_stages.sql`**
  - Creates `@customer_data_stage` (internal, Parquet format)
  - Creates `@transaction_data_stage` (future use)

### 2. dbt Bronze Models
- **`/dbt_customer_analytics/models/bronze/raw_customers.sql`**
  - Loads Parquet from `@customer_data_stage`
  - Incremental materialization (unique_key: customer_id)
  - COPY INTO with metadata capture

- **`/dbt_customer_analytics/models/bronze/raw_transactions.sql`**
  - Loads GZIP CSV from `@transaction_stage_historical`
  - Incremental materialization (unique_key: transaction_id)
  - COPY INTO with metadata capture

- **`/dbt_customer_analytics/models/bronze/schema.yml`**
  - Model documentation
  - Column tests (unique, not_null, accepted_values)

### 3. Documentation
- **`/docs/architecture_diagram.md`** (updated)
  - Shows dbt-managed Bronze layer
  - Updated data flow diagrams
  - New Quick Start Guide

- **`/docs/dbt_bronze_refactor_summary.md`** (this file)

---

## Files Modified

### 1. Snowpark Stored Procedure
- **`/snowflake/procedures/generate_customers.sql`**
  - **Before**: `session.write_pandas()` â†’ direct write to table
  - **After**: `snowpark_df.write.parquet()` â†’ write to stage
  - Returns stage location for dbt ingestion
  - Versioned filenames: `customers_seed42_20251117_143052.parquet`

### 2. dbt Sources
- **`/dbt_customer_analytics/models/staging/_staging_sources.yml`**
  - Updated descriptions to reflect dbt Bronze models
  - Changed from "S3 CSV" to "internal/external stages"
  - Updated load methods

---

## New Workflow

### Setup (One-Time)

```bash
# 1. Create internal stages
snowsql -f snowflake/setup/09_create_internal_stages.sql

# 2. Verify stages exist
snowsql -q "LIST @CUSTOMER_ANALYTICS.BRONZE.customer_data_stage;"
```

### Generate & Load Customers

```sql
-- Step 1: Generate customer data to stage
CALL BRONZE.GENERATE_CUSTOMERS(50000, 42);

-- Step 2: Load from stage to Bronze table
-- (Run from dbt project directory)
```

```bash
dbt run --select bronze.raw_customers
```

### Load Transactions (from S3)

```bash
# Assumes transaction CSVs already in S3
dbt run --select bronze.raw_transactions
```

### Build Full Pipeline

```bash
# Run entire pipeline: Bronze â†’ Silver â†’ Gold
dbt run

# Run tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

---

## Data Flow Details

### Customers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Snowpark Stored Procedure                                â”‚
â”‚    CALL BRONZE.GENERATE_CUSTOMERS(50000, 42)                â”‚
â”‚    â€¢ Generates 50K synthetic customers                      â”‚
â”‚    â€¢ Uses Faker, NumPy, Pandas                              â”‚
â”‚    â€¢ Runs entirely in Snowflake                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Internal Stage                                            â”‚
â”‚    @customer_data_stage/customers_seed42_20251117.parquet   â”‚
â”‚    â€¢ Parquet format (efficient compression)                 â”‚
â”‚    â€¢ Versioned by seed & timestamp                          â”‚
â”‚    â€¢ Replayable (same seed = same data)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. dbt Bronze Model                                          â”‚
â”‚    bronze.raw_customers                                      â”‚
â”‚    â€¢ COPY INTO from Parquet                                 â”‚
â”‚    â€¢ Incremental (skip loaded files)                        â”‚
â”‚    â€¢ Captures metadata (source_file, row_number)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Bronze Table                                              â”‚
â”‚    BRONZE.RAW_CUSTOMERS                                      â”‚
â”‚    â€¢ 50,000 rows                                             â”‚
â”‚    â€¢ 16 columns (13 data + 3 metadata)                      â”‚
â”‚    â€¢ Ready for dbt staging models                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transactions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. External Data Generation                                  â”‚
â”‚    (Outside Snowflake)                                       â”‚
â”‚    â€¢ Generate transaction CSVs                               â”‚
â”‚    â€¢ GZIP compress                                           â”‚
â”‚    â€¢ Upload to S3                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. External S3 Stage                                         â”‚
â”‚    @transaction_stage_historical                             â”‚
â”‚    â€¢ s3://bucket/transactions/historical/*.csv.gz           â”‚
â”‚    â€¢ CSV format with GZIP compression                        â”‚
â”‚    â€¢ Storage integration for auth                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. dbt Bronze Model                                          â”‚
â”‚    bronze.raw_transactions                                   â”‚
â”‚    â€¢ COPY INTO from GZIP CSV                                â”‚
â”‚    â€¢ Incremental (skip loaded files)                        â”‚
â”‚    â€¢ Captures metadata (source_file, row_number)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Bronze Table                                              â”‚
â”‚    BRONZE.RAW_TRANSACTIONS                                   â”‚
â”‚    â€¢ ~13.5M rows                                             â”‚
â”‚    â€¢ 11 columns (8 data + 3 metadata)                       â”‚
â”‚    â€¢ Ready for dbt staging models                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## dbt DAG (Data Lineage)

```
@customer_data_stage (Parquet)
  â†“
bronze.raw_customers
  â†“
silver.stg_customers
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â†“                       â†“                       â†“
gold.dim_customer    gold.customer_360_profile  gold.customer_segments
```

```
@transaction_stage_historical (CSV.GZ)
  â†“
bronze.raw_transactions
  â†“
silver.stg_transactions
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â†“                       â†“                       â†“
gold.fct_transactions  gold.customer_360_profile  gold.metrics
```

---

## Testing

### Bronze Layer Tests

```bash
# Test Bronze models only
dbt test --select bronze

# Test specific model
dbt test --select bronze.raw_customers

# View test results
dbt test --store-failures
```

**Tests Included:**
- `unique` on customer_id, transaction_id
- `not_null` on critical fields
- `accepted_values` on categorical fields
- `dbt_utils.recency` on ingestion_timestamp (in sources.yml)

---

## Comparison: Old vs New

| Aspect | Before | After |
|--------|--------|-------|
| **Bronze Management** | Manual SQL scripts | dbt models |
| **Customer Load** | Direct write to table | Stage â†’ dbt COPY INTO |
| **Transaction Load** | Manual SQL script | dbt COPY INTO |
| **Orchestration** | Manual (run scripts in order) | `dbt run` (DAG-based) |
| **Testing** | None | dbt tests on Bronze |
| **Lineage** | Starts at Silver | Full Bronze â†’ Gold |
| **Incremental Loads** | Manual FORCE=FALSE | dbt incremental built-in |
| **Documentation** | Separate SQL comments | dbt docs (schema.yml) |
| **Replayability** | Truncate + reload | Incremental (idempotent) |

---

## Migration Path (If Needed)

If you have existing data in Bronze tables created by old scripts:

### Option 1: Fresh Start
```sql
-- Drop old Bronze tables
DROP TABLE BRONZE.RAW_CUSTOMERS;
DROP TABLE BRONZE.BRONZE_TRANSACTIONS;  -- Note different name

-- Run dbt to create new ones
dbt run --select bronze
```

### Option 2: Rename & Keep Old Data
```sql
-- Rename old tables
ALTER TABLE BRONZE.RAW_CUSTOMERS RENAME TO BRONZE.RAW_CUSTOMERS_OLD;
ALTER TABLE BRONZE.BRONZE_TRANSACTIONS RENAME TO BRONZE.RAW_TRANSACTIONS_OLD;

-- Run dbt to create new tables
dbt run --select bronze

-- Optionally copy old data
INSERT INTO BRONZE.RAW_CUSTOMERS SELECT * FROM BRONZE.RAW_CUSTOMERS_OLD;
INSERT INTO BRONZE.RAW_TRANSACTIONS SELECT * FROM BRONZE.RAW_TRANSACTIONS_OLD;
```

---

## Rollback Plan

If you need to revert to the old approach:

1. **Stop using dbt Bronze models**
   ```bash
   # Don't run bronze models
   dbt run --exclude bronze
   ```

2. **Use old SQL load scripts**
   ```sql
   -- For customers: modify procedure to write directly to table
   -- For transactions: use snowflake/load/load_transactions_bulk.sql
   ```

3. **Update staging models**
   ```sql
   -- Change from: {{ source('bronze', 'raw_customers') }}
   -- To: {{ ref('bronze_customers_table') }}
   ```

---

## Performance Considerations

### Parquet vs CSV
- âœ… **Parquet**: Better compression, faster reads, columnar storage
- ğŸ“Š **Size**: Parquet ~10x smaller than CSV
- âš¡ **Load Speed**: Parquet ~3x faster

### Incremental Strategy
- First run: Full COPY INTO (loads all files)
- Subsequent runs: Only new files (FORCE=FALSE)
- dbt tracks loaded files automatically

### Warehouse Sizing
- **Bronze ingestion**: SMALL warehouse (50K customers = ~5 seconds)
- **Full dbt run**: MEDIUM warehouse (includes 13.5M transactions)

---

## Next Steps

1. âœ… **Refactoring Complete**
   - All code updated
   - Documentation updated
   - Architecture diagram updated

2. ğŸ”„ **Testing** (Recommended)
   ```bash
   # Test the new workflow end-to-end
   CALL BRONZE.GENERATE_CUSTOMERS(1000, 123);  # Small test
   dbt run --select bronze.raw_customers
   dbt test --select bronze
   ```

3. ğŸ“Š **Production Deployment**
   ```sql
   -- Generate full dataset
   CALL BRONZE.GENERATE_CUSTOMERS(50000, 42);

   -- Load Bronze and build pipeline
   dbt run
   dbt test
   ```

4. ğŸ¤– **Automation** (Optional)
   - Schedule dbt runs in Snowflake Tasks
   - Set up dbt Cloud (if using)
   - Add monitoring/alerting

---

## Key Takeaways

âœ¨ **Main Achievement**: Full dbt lineage from Bronze â†’ Gold

ğŸ¯ **Consistency**: Both customers & transactions use stage â†’ COPY INTO pattern

ğŸ“Š **Governance**: dbt owns all data transformations and testing

ğŸ”„ **Maintainability**: Single `dbt run` command replaces multiple manual scripts

ğŸ“– **Documentation**: Auto-generated docs with full DAG visualization

---

**Author**: Data Engineering Team
**Date**: 2025-11-17
**Version**: 1.0
